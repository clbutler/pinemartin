Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 4
Rules claiming more threads will be scaled down.
Conda environments: ignored
Job stats:
job                            count
---------------------------  -------
clean_pine_martin_sightings        1
total                              1

Select jobs to execute...
Execute 1 jobs...

[Wed Sep  4 23:37:49 2024]
localrule clean_pine_martin_sightings:
    input: outputs/pine_martins_sightings.csv
    output: outputs/pine_martins_sightings_clean.csv, outputs/pine_martins_sightings_plot.pdf
    jobid: 0
    reason: Forced execution
    resources: tmpdir=/var/folders/1z/9q2sbs594sj783bxhkj9gfl80000gn/T

RuleException:
WorkflowError in file /Users/chrisbutler/Documents/pine_martin/pine_martin, line 18:
Failed to open source file /Users/chrisbutler/Documents/pine_martin/pm_clean.py
FileNotFoundError: [Errno 2] No such file or directory: '/Users/chrisbutler/Documents/pine_martin/pm_clean.py'
[Wed Sep  4 23:37:49 2024]
Error in rule clean_pine_martin_sightings:
    jobid: 0
    input: outputs/pine_martins_sightings.csv
    output: outputs/pine_martins_sightings_clean.csv, outputs/pine_martins_sightings_plot.pdf
    conda-env: /Users/chrisbutler/Documents/pine_martin/.snakemake/conda/77455e3e2502af1ae574a5776ceb467e_

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2024-09-04T233749.179769.snakemake.log
WorkflowError:
At least one job did not complete successfully.
